{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828fad35-1d19-4539-b3ed-864986211a09",
   "metadata": {},
   "source": [
    "# Generic Evaluation of Any Text File\n",
    "\n",
    "**Prerequisites**\n",
    "- obtain training files in NT format (see notebook 'RDF2Vec KBC Steps.ipynb')\n",
    "- train your embeddings so that you have a txt file\n",
    "- install kbc_rdf2vec ([https://github.com/janothan/kbc_rdf2vec](https://github.com/janothan/kbc_rdf2vec))\n",
    "- install kbc_evaluation ([https://github.com/janothan/kbc_evaluation/](https://github.com/janothan/kbc_evaluation/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218a3a48-635a-4b69-ae95-2786932ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_dir = \"/work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove\"\n",
    "\n",
    "fb_vector_txt_file = \"./kglove_fb/vectors.txt\"\n",
    "wn_vector_txt_file = \"./kglove_wn/vectors.txt\"\n",
    "\n",
    "is_vectors_contain_predicates = False\n",
    "\n",
    "# file where \"WN18.nt\" and \"FB15k.nt\" can be found\n",
    "nt_dir = \"/work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_2/nt_files\"\n",
    "\n",
    "# Now let's decide on your directory where everything shall be written to (requires > 5Gb of disk space)\n",
    "working_directory = \"/work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f62e14-fb95-4b95-b5e6-730540965f48",
   "metadata": {},
   "source": [
    "We first need to tranform the file in the gensim kv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efb928cf-3453-4bef-ac9f-e5abf753270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-24 22:26:20,100 - gensim.scripts.glove2word2vec - INFO - converting 16297 vectors from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_txt_no_tags.txt to /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.w2v\n",
      "2021-08-24 22:26:20,436 - gensim.models.utils_any2vec - INFO - loading projection weights from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.w2v\n",
      "2021-08-24 22:26:25,396 - gensim.models.utils_any2vec - INFO - loaded (16297, 200) matrix from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.w2v\n",
      "2021-08-24 22:26:25,398 - gensim.utils - INFO - saving Word2VecKeyedVectors object under /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.kv, separately None\n",
      "2021-08-24 22:26:25,399 - gensim.utils - INFO - not storing attribute vectors_norm\n",
      "2021-08-24 22:26:25,875 - gensim.utils - INFO - saved /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.kv\n",
      "2021-08-24 22:26:39,392 - gensim.scripts.glove2word2vec - INFO - converting 40962 vectors from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_txt_no_tags.txt to /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.w2v\n",
      "2021-08-24 22:26:40,231 - gensim.models.utils_any2vec - INFO - loading projection weights from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.w2v\n",
      "2021-08-24 22:26:51,992 - gensim.models.utils_any2vec - INFO - loaded (40962, 200) matrix from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.w2v\n",
      "2021-08-24 22:26:51,995 - gensim.utils - INFO - saving Word2VecKeyedVectors object under /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.kv, separately None\n",
      "2021-08-24 22:26:51,996 - gensim.utils - INFO - not storing attribute vectors_norm\n",
      "2021-08-24 22:26:53,123 - gensim.utils - INFO - saved /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.kv\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "def remove_tags(txt_file: str, file_to_write_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Returns the newly written file where the concepts are not enclosed in tags (<...>).\n",
    "    The new file will be written to the working directory.\n",
    "    \"\"\"\n",
    "        \n",
    "    with file_to_write_path.open(mode=\"w+\") as file_to_write:\n",
    "        with Path(txt_file).open(mode='r') as file_to_read:\n",
    "            for line in file_to_read:\n",
    "                starting_token = \"\"\n",
    "                tokens = line.split(\" \")\n",
    "                if tokens[0].startswith(\"<\") and tokens[0].endswith(\">\"):\n",
    "                    starting_token = tokens[0][1:len(tokens[0])-1]\n",
    "                else:\n",
    "                    starting_token = tokens[0]\n",
    "                file_to_write.write(starting_token)\n",
    "                for i in range(1, len(tokens)):\n",
    "                    file_to_write.write(\" \" + tokens[i])\n",
    "    \n",
    "    return str(file_to_write_path.resolve())\n",
    "                    \n",
    "        \n",
    "\n",
    "def convert_to_kv(txt_file: str, new_file: str) -> Word2VecKeyedVectors:\n",
    "    w2v_file = new_file[0:len(new_file)-3] + \".w2v\"\n",
    "    if Path(new_file).is_file():\n",
    "        print(f\"WARNING: File {new_file} exists already! Just loading the file...\")\n",
    "        return KeyedVectors.load_word2vec_format(w2v_file)\n",
    "    else:\n",
    "        glove2word2vec(txt_file, w2v_file)\n",
    "    result = KeyedVectors.load_word2vec_format(w2v_file)\n",
    "    result.save(new_file)\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "# kv files to be written\n",
    "fb_kv_file = str(Path(working_directory).joinpath(\"fb_kv_format.kv\").resolve())\n",
    "wn_kv_file = str(Path(working_directory).joinpath(\"wn_kv_format.kv\").resolve())\n",
    "\n",
    "# write kv files (and also files where no tags are around the concepts)\n",
    "fb_no_tags = Path(working_directory).joinpath(\"fb_txt_no_tags.txt\")\n",
    "wn_no_tags = Path(working_directory).joinpath(\"wn_txt_no_tags.txt\")\n",
    "fb_kv = convert_to_kv(remove_tags(txt_file = fb_vector_txt_file, file_to_write_path=fb_no_tags), fb_kv_file)\n",
    "wn_kv = convert_to_kv(remove_tags(txt_file = wn_vector_txt_file, file_to_write_path=wn_no_tags), wn_kv_file)\n",
    "\n",
    "# let's set the training files (required for predictions)...\n",
    "wn_nt_path = str(Path(nt_dir).joinpath(\"WN18.nt\").resolve())\n",
    "fb15k_nt_path = str(Path(nt_dir).joinpath(\"FB15k.nt\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc3f5dee-edf6-4e80-b7e7-c0f98b353197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2654b75-dcef-4b7d-a4e3-a271bfe516d2",
   "metadata": {},
   "source": [
    "## Let's predict!\n",
    "We start by generating the files containing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb801d2-84ae-49c1-a31c-1797e53da6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-24 22:27:28,110 - kbc_rdf2vec.rdf2vec_kbc - INFO - Gensim vector file detected.\n",
      "2021-08-24 22:27:28,111 - gensim.utils - INFO - loading Word2VecKeyedVectors object from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.kv\n",
      "2021-08-24 22:27:28,617 - gensim.utils - INFO - setting ignored attribute vectors_norm to None\n",
      "2021-08-24 22:27:28,619 - gensim.utils - INFO - loaded /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/wn_kv_format.kv\n",
      "2021-08-24 22:27:29,192 - kbc_rdf2vec.prediction - INFO - Initializing AveragePredicatePredictionFunction\n",
      "Predicting Tails and Heads\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]2021-08-24 22:27:31,769 - gensim.models.keyedvectors - INFO - precomputing L2-norms of word weight vectors\n",
      "100%|██████████| 5000/5000 [16:47<00:00,  4.96it/s]\n",
      "2021-08-24 22:44:19,535 - kbc_rdf2vec.rdf2vec_kbc - INFO - Erroneous Triples: 0\n",
      "2021-08-24 22:44:19,655 - kbc_rdf2vec.rdf2vec_kbc - INFO - Gensim vector file detected.\n",
      "2021-08-24 22:44:19,660 - gensim.utils - INFO - loading Word2VecKeyedVectors object from /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.kv\n",
      "2021-08-24 22:44:19,870 - gensim.utils - INFO - setting ignored attribute vectors_norm to None\n",
      "2021-08-24 22:44:19,871 - gensim.utils - INFO - loaded /work/jportisc/kbc_rdf2vec/strategy_grid_2/evaluation_3_kglove/fb_kv_format.kv\n",
      "2021-08-24 22:44:22,210 - kbc_rdf2vec.prediction - INFO - Initializing AveragePredicatePredictionFunction\n",
      "Predicting Tails and Heads\n",
      "  0%|          | 0/59071 [00:00<?, ?it/s]2021-08-24 22:44:28,257 - gensim.models.keyedvectors - INFO - precomputing L2-norms of word weight vectors\n",
      "100%|██████████| 59071/59071 [1:05:51<00:00, 14.95it/s]\n",
      "2021-08-24 23:50:20,040 - kbc_rdf2vec.rdf2vec_kbc - INFO - Erroneous Triples: 0\n"
     ]
    }
   ],
   "source": [
    "from kbc_rdf2vec.dataset import DataSet\n",
    "from kbc_rdf2vec.prediction import PredictionFunctionEnum, PredictionFunction\n",
    "from kbc_rdf2vec.rdf2vec_kbc import Rdf2vecKbc\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_prediction_files() -> None:\n",
    "    wn_vector_file = wn_kv_file\n",
    "    wn_nt_file = wn_nt_path\n",
    "    fb15k_vector_file = fb_kv_file\n",
    "    fb15k_nt_file = fb15k_nt_path\n",
    "\n",
    "    # let's make a directory if it does not exist yet\n",
    "    prediction_path = os.path.join(working_directory, \"predictions\")\n",
    "    if not os.path.exists(prediction_path):\n",
    "        os.makedirs(prediction_path)\n",
    "    \n",
    "    if is_vectors_contain_predicates:\n",
    "        # ANN WN\n",
    "        kbc = Rdf2vecKbc(\n",
    "            model_path=wn_vector_file,\n",
    "            data_set=DataSet.WN18,\n",
    "            n=None,\n",
    "            prediction_function=PredictionFunctionEnum.ANN,\n",
    "            file_for_predicate_exclusion=wn_nt_file,\n",
    "            is_reflexive_match_allowed=False,\n",
    "        )\n",
    "        kbc.predict(os.path.join(prediction_path, \"wn_ann.txt\"))\n",
    "\n",
    "        # ANN FB\n",
    "        kbc = Rdf2vecKbc(\n",
    "            model_path=fb15k_vector_file,\n",
    "            data_set=DataSet.FB15K,\n",
    "            n=None,\n",
    "            prediction_function=PredictionFunctionEnum.ANN,\n",
    "            file_for_predicate_exclusion=fb15k_nt_file,\n",
    "            is_reflexive_match_allowed=False,\n",
    "        )\n",
    "        kbc.predict(os.path.join(prediction_path, \"fb15k_ann.txt\"))\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    # most similar WN\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=wn_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.WN18,\n",
    "        file_for_predicate_exclusion=wn_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.MOST_SIMILAR,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"wn_most_similar.txt\"))\n",
    "    \n",
    "    # most similar FB\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.MOST_SIMILAR,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_most_similar.txt\"))\n",
    "    \n",
    "    # avg most similar WN\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=wn_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.WN18,\n",
    "        file_for_predicate_exclusion=wn_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.PREDICATE_AVERAGING_MOST_SIMILAR,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"wn_averaged_most_similar.txt\"))\n",
    "\n",
    "    # avg most similar FB\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.PREDICATE_AVERAGING_MOST_SIMILAR,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_averaged_most_similar.txt\")) \n",
    "    \n",
    "    # addition WN\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=wn_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.WN18,\n",
    "        file_for_predicate_exclusion=wn_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"wn_addition.txt\"))\n",
    "\n",
    "    # addition FB\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_addition.txt\"))\n",
    "    \n",
    "    # addition FB with reflexive matches allowed\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=True,\n",
    "        prediction_function=PredictionFunctionEnum.ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_reflexive_addition.txt\"))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # avg addition WN\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=wn_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.WN18,\n",
    "        file_for_predicate_exclusion=wn_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.PREDICATE_AVERAGING_ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"wn_averaged_addition.txt\"))\n",
    "\n",
    "    # avg addition FB\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=False,\n",
    "        prediction_function=PredictionFunctionEnum.PREDICATE_AVERAGING_ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_averaged_addition.txt\"))\n",
    "    \n",
    "    \"\"\"\n",
    "    # avg addition FB with reflexive matches allowed\n",
    "    kbc = Rdf2vecKbc(\n",
    "        model_path=fb15k_vector_file,\n",
    "        n=None,\n",
    "        data_set=DataSet.FB15K,\n",
    "        file_for_predicate_exclusion=fb15k_nt_file,\n",
    "        is_reflexive_match_allowed=True,\n",
    "        prediction_function=PredictionFunctionEnum.PREDICATE_AVERAGING_ADDITION,\n",
    "    )\n",
    "    kbc.predict(os.path.join(prediction_path, \"fb15k_reflexive_averaged_addition.txt\"))\n",
    "    \"\"\"\n",
    "\n",
    "generate_prediction_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f63833-152a-44e9-88e2-0314717ab959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18fe70e-0ee4-475c-bc17-035a8a970e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
